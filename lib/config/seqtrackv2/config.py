from easydict import EasyDict as edict
import yaml

cfg = edict()

# MODEL
cfg.MODEL = edict()
cfg.MODEL.HIDDEN_DIM = 256 # hidden dimension for the decoder and vocabulary
cfg.MODEL.BINS = 4000 # number of discrete bins
cfg.MODEL.FEATURE_TYPE = "x" # the input feature to decoder. x, xz, or token
cfg.MODEL.INTERFACE_TYPE = 'low-rank_add'
cfg.MODEL.INTERFACE_DIM = 8

# MODEL.LANGUAGE
cfg.MODEL.LANGUAGE = edict()
cfg.MODEL.LANGUAGE.IMPLEMENT = 'pytorch'
cfg.MODEL.LANGUAGE.TYPE = 'bert-base-uncased'
cfg.MODEL.LANGUAGE.PATH = 'pretrained/bert/bert-base-uncased.tar.gz'
cfg.MODEL.LANGUAGE.VOCAB_PATH = 'pretrained/bert/bert-base-uncased-vocab.txt'
cfg.MODEL.LANGUAGE.POOLING = 'mean' # max, mean, None
# BERT
cfg.MODEL.LANGUAGE.BERT = edict()
# cfg.MODEL.LANGUAGE.BERT.LR = 0.00001
cfg.MODEL.LANGUAGE.BERT.ENC_NUM = 12
cfg.MODEL.LANGUAGE.BERT.HIDDEN_DIM = 256
cfg.MODEL.LANGUAGE.BERT.MAX_QUERY_LEN = 40

# MODEL.ENCODER
# for more customization for encoder, please modify lib/models/seqtrack/vit.py
cfg.MODEL.ENCODER = edict()
cfg.MODEL.ENCODER.TYPE = "vit_base_patch16" # encoder model
cfg.MODEL.ENCODER.DROP_PATH = 0
cfg.MODEL.ENCODER.PRETRAIN_TYPE = "mae" # mae, default, or scratch
cfg.MODEL.ENCODER.STRIDE = 16
cfg.MODEL.ENCODER.USE_CHECKPOINT = False # to save the memory.
cfg.MODEL.ENCODER.INSTRUCT = True #True: using instruct for encoder
# MODEL.DECODER
cfg.MODEL.DECODER = edict()
cfg.MODEL.DECODER.NHEADS = 8
cfg.MODEL.DECODER.DROPOUT = 0.1
cfg.MODEL.DECODER.DIM_FEEDFORWARD = 1024
cfg.MODEL.DECODER.DEC_LAYERS = 6
cfg.MODEL.DECODER.PRE_NORM = False
cfg.MODEL.DECODER.INSTRUCT = True #True: using instruct tokens, False: using START token for all modalities

# TRAIN
cfg.TRAIN = edict()
cfg.TRAIN.LR = 0.0001
cfg.TRAIN.WEIGHT_DECAY = 0.0001
cfg.TRAIN.EPOCH = 500
cfg.TRAIN.LR_DROP_EPOCH = 400
cfg.TRAIN.BATCH_SIZE = 8
cfg.TRAIN.NUM_WORKER = 8
cfg.TRAIN.OPTIMIZER = "ADAMW"
cfg.TRAIN.ENCODER_MULTIPLIER = 0.1  # encoder's LR = this factor * LR
cfg.TRAIN.FREEZE_ENCODER = False # for freezing the parameters of encoder
cfg.TRAIN.ENCODER_OPEN = [] # only for debug, open some layers of encoder when FREEZE_ENCODER is True
cfg.TRAIN.CE_WEIGHT = 1.0 # weight for cross-entropy loss
cfg.TRAIN.PRINT_INTERVAL = 50 # interval to print the training log
cfg.TRAIN.GRAD_CLIP_NORM = 0.1
cfg.TRAIN.FIX_BN = False
# TRAIN.SCHEDULER
cfg.TRAIN.SCHEDULER = edict()
cfg.TRAIN.SCHEDULER.TYPE = "step"
cfg.TRAIN.SCHEDULER.DECAY_RATE = 0.1
cfg.TRAIN.SCHEDULER.T_MAX = 240 # for cosine scheduler
cfg.TRAIN.SCHEDULER.NUM_WARMUP_STEPS = 200
cfg.TRAIN.SCHEDULER.NUM_TRAINING_STEPS = 200000 # just for cosine scheduler
cfg.TRAIN.SCHEDULER.NUM_CYCLES = 1 # just for cosine scheduler
cfg.TRAIN.TYPE = "peft" # peft or fft
cfg.TRAIN.PRETRAINED_PATH = None

# DATA
cfg.DATA = edict()
cfg.DATA.MEAN = [0.485, 0.456, 0.406]
cfg.DATA.STD = [0.229, 0.224, 0.225]
cfg.DATA.MAX_SAMPLE_INTERVAL = 200
cfg.DATA.SAMPLER_MODE = "causual"
cfg.DATA.LOADER = "tracking"
cfg.DATA.SEQ_FORMAT = "xywh"
cfg.DATA.MULTI_MODAL_VISION = True # vision multi-modal
cfg.DATA.MULTI_MODAL_LANGUAGE = True # language multi-modal
# DATA.TRAIN
cfg.DATA.TRAIN = edict()
cfg.DATA.TRAIN.DATASETS_NAME = ["MY_DATA"]
cfg.DATA.TRAIN.DATASETS_RATIO = [1]
cfg.DATA.TRAIN.SAMPLE_PER_EPOCH = 60000
# DATA.SEARCH
cfg.DATA.SEARCH = edict()
cfg.DATA.SEARCH.NUMBER = 1  #number of search region, only support 1 for now.
cfg.DATA.SEARCH.SIZE = 256
cfg.DATA.SEARCH.FACTOR = 4.0
cfg.DATA.SEARCH.CENTER_JITTER = 3.5
cfg.DATA.SEARCH.SCALE_JITTER = 0.5
# DATA.TEMPLATE
cfg.DATA.TEMPLATE = edict()
cfg.DATA.TEMPLATE.NUMBER = 1
cfg.DATA.TEMPLATE.SIZE = 256
cfg.DATA.TEMPLATE.FACTOR = 4.0
cfg.DATA.TEMPLATE.CENTER_JITTER = 0
cfg.DATA.TEMPLATE.SCALE_JITTER = 0

# TEST
cfg.TEST = edict()
cfg.TEST.TEMPLATE_FACTOR = 4.0
cfg.TEST.TEMPLATE_SIZE = 256
cfg.TEST.SEARCH_FACTOR = 4.0
cfg.TEST.SEARCH_SIZE = 256
cfg.TEST.EPOCH = 500
cfg.TEST.WINDOW = False # window penalty
cfg.TEST.NUM_TEMPLATES = 1

cfg.TEST.UPDATE_INTERVALS = edict()
cfg.TEST.UPDATE_INTERVALS.DEFAULT = 25

cfg.TEST.UPDATE_THRESHOLD = edict()
cfg.TEST.UPDATE_THRESHOLD.DEFAULT = 0.475

cfg.TEST.MULTI_MODAL_VISION = edict()
cfg.TEST.MULTI_MODAL_VISION.DEFAULT = True
cfg.TEST.MULTI_MODAL_VISION.DEPTHTRACK = True
cfg.TEST.MULTI_MODAL_VISION.LASHER = True
cfg.TEST.MULTI_MODAL_VISION.VISEVENT = True
cfg.TEST.MULTI_MODAL_VISION.OTB99_LANG = True
cfg.TEST.MULTI_MODAL_VISION.TNL2K = True
cfg.TEST.MULTI_MODAL_VISION.LASOT_LANG = True





def _edict2dict(dest_dict, src_edict):
    if isinstance(dest_dict, dict) and isinstance(src_edict, dict):
        for k, v in src_edict.items():
            if not isinstance(v, edict):
                dest_dict[k] = v
            else:
                dest_dict[k] = {}
                _edict2dict(dest_dict[k], v)
    else:
        return


def gen_config(config_file):
    cfg_dict = {}
    _edict2dict(cfg_dict, cfg)
    with open(config_file, 'w') as f:
        yaml.dump(cfg_dict, f, default_flow_style=False)


def _update_config(base_cfg, exp_cfg):
    if isinstance(base_cfg, dict) and isinstance(exp_cfg, edict):
        for k, v in exp_cfg.items():
            if k in base_cfg:
                if not isinstance(v, dict):
                    base_cfg[k] = v
                else:
                    _update_config(base_cfg[k], v)
            else:
                raise ValueError("{} not exist in config.py".format(k))
    else:
        return


def update_config_from_file(filename):
    exp_config = None
    with open(filename) as f:
        exp_config = edict(yaml.safe_load(f))
        _update_config(cfg, exp_config)


