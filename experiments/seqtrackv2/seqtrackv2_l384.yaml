DATA:
  MAX_SAMPLE_INTERVAL: 400
  MEAN:
  - 0.485
  - 0.456
  - 0.406
  SEARCH:
    CENTER_JITTER: 3.5
    FACTOR: 4.0
    SCALE_JITTER: 0.5
    SIZE: 384
    NUMBER: 1
  STD:
  - 0.229
  - 0.224
  - 0.225
  TEMPLATE:
    CENTER_JITTER: 0
    FACTOR: 4.0
    SCALE_JITTER: 0
    SIZE: 384
    NUMBER: 2
  TRAIN:
    DATASETS_NAME:
    - MY_DATA
    DATASETS_RATIO:
    - 1
    SAMPLE_PER_EPOCH: 12000
MODEL:
  LANGUAGE:
    TYPE: 'bert-base-uncased'
    PATH: 'pretrained/bert/bert-base-uncased.tar.gz'
    VOCAB_PATH: 'pretrained/bert/bert-base-uncased-vocab.txt'
    BERT:
#      LR: 0.00001
      ENC_NUM: 12
      HIDDEN_DIM: 256
      MAX_QUERY_LEN: 40
  ENCODER:
    TYPE: vitmm_large_patch16
    PRETRAIN_TYPE: mae
    INSTRUCT: True # task-prompt token
  DECODER:
    DEC_LAYERS: 2
    INSTRUCT: True # task-prompt token
  HIDDEN_DIM: 256
  BINS: 4000
  FEATURE_TYPE: x
  INTERFACE_TYPE: low-rank_add
  INTERFACE_DIM: 32
TRAIN:
  BATCH_SIZE: 2
  EPOCH: 240
  GRAD_CLIP_NORM: 0.1
  CE_WEIGHT: 1.0
  LR: 0.0004
  LR_DROP_EPOCH: 192
  NUM_WORKER: 8
  OPTIMIZER: ADAMW
  PRINT_INTERVAL: 50
  SCHEDULER:
    TYPE: cosine
    DECAY_RATE: 0.1
    T_MAX : 240
  WEIGHT_DECAY: 0.0001
  TYPE: peft
  FIX_BN: True
  PRETRAINED_PATH: 'pretrained/seqtrack/seqtrack_l384/SEQTRACK_ep0500.pth.tar'
TEST:
  EPOCH: 240
  SEARCH_FACTOR: 4.0
  SEARCH_SIZE: 384
  TEMPLATE_FACTOR: 4.0
  TEMPLATE_SIZE: 384
  WINDOW: true
  NUM_TEMPLATES: 2

